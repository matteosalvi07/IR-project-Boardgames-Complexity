{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complexity in Boardgames dComplexity in Boardgames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rulebook:\n",
    "    def __init__(self, filename, text, pagenum,num_sent,num_word,num_complex_word,syn_analysis):\n",
    "        self.filename = filename\n",
    "        self.text = text\n",
    "        self.pagenum= pagenum\n",
    "        self.num_word=num_word\n",
    "        self.num_sent=num_sent\n",
    "        self.num_complex_word=num_complex_word\n",
    "        self.readability=0.4*(self.num_word/self.num_sent + 100*self.num_complex_word/self.num_word) #readability metric\n",
    "        self.syn_analysis=syn_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoardGame:\n",
    "\n",
    "    def __init__(self, name, comments,weight,expansions_num,rulebook,score):\n",
    "        self.name = name\n",
    "        self.comments = comments\n",
    "        self.BGGweight=weight\n",
    "        self.expansions_num = expansions_num\n",
    "        self.rulebook=rulebook\n",
    "        self.comment_score=score\n",
    "        self.total_weight=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy_syllables import SpacySyllables\n",
    "import spacy_fastlang\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "import xml.etree.ElementTree as et\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from string import punctuation\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy_fastlang.LanguageDetector at 0x1e3e9aa94c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('english')\n",
    "bggxmlapi2 = 'https://boardgamegeek.com/xmlapi2/'\n",
    "bggxmlapi = 'http://www.boardgamegeek.com/xmlapi/'\n",
    "\n",
    "\n",
    "mypath = YOUR PATH WITH PDF,
    "easy= \"accessible,clear,effortless,obvious,painless,simple,smooth,straightforward,uncomplicated,apparent,basic,child's play,cinch,easily done,elementary,evident,facile,inconsiderable,light,little,manageable,manifest,mere,no bother,no problem,no sweat,no trouble,not burdensome,nothing to it,paltry,picnic,piece of cake,plain,plain sailing,pushover,royal,simple as ABC,slight,snap,undemanding,uninvolved,untroublesome,wieldy,yielding,easy,\"\n",
    "difficult=\"difficult,ambitious,arduous,burdensome,challenging,crucial,demanding,laborious,onerous,painful,problematic,severe,strenuous,tough,troublesome,backbreaker,bothersome,difficile,easier said than done,effortful,exacting,formidable,galling,gargantuan,hard-won,heavy,herculean,immense,intricate,irritating,labored,no picnic,not easy,operose,problem,prohibitive,rigid,stiff,titanic,toilsome,trying,unyielding,uphill,upstream,wearisome,\"\n",
    "difficult_word=[option.strip() for option in difficult.split(',')]\n",
    "easy_word=[option.strip() for option in easy.split(',')]\n",
    "\n",
    "\n",
    "#loaded the model for nlp with spacy and the pipelines\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(\"syllables\", after=\"tagger\")\n",
    "nlp.add_pipe(\"language_detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(): #to read file in a directory\n",
    "    return [f for f in listdir(mypath) if isfile(join(mypath, f))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rolebook_pdf(file): #read the content of a pdf to create an istance of a rulebook\n",
    "    reader = PdfReader(mypath + file)\n",
    "    filename = file\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "\n",
    "    corpus = [sentence for sentence in nlp(text).sents]\n",
    "    df = process_text(corpus)\n",
    "    num_sent = len(text)\n",
    "    num_word = df.query('alpha==True').shape[0]\n",
    "    num_complex_word = df.query('alpha==True&syll_count>=3').shape[0]\n",
    "    pos = df.groupby([\"pos\"]).size()\n",
    "    syn_analysis = synt_analysis(pos)\n",
    "    return Rulebook(filename, text, reader.getNumPages(), num_sent, num_word, num_complex_word, syn_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(corpus): #process rulebook content to the sintactic analysis\n",
    "    tokens = []\n",
    "    for sentence in corpus:\n",
    "        for token in sentence:\n",
    "            tokens.append({\n",
    "                'position': token.idx, 'text': token.text, 'pos': token.pos_, 'lemma': token.lemma_,\n",
    "                'alpha': token.is_alpha, 'stop': token.is_stop,\n",
    "                'morph': token.morph, 'syll_count': token._.syllables_count\n",
    "            })\n",
    "    return pd.DataFrame(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synt_analysis(pos): #metric from syntactic analysis\n",
    "    aggettivi = pos['ADJ'] + pos['NUM']\n",
    "    nomi = pos['NOUN'] + pos['PROPN']\n",
    "    pronomi = pos['PRON']\n",
    "    spazi = pos['SPACE']\n",
    "    verbi = pos['VERB']\n",
    "    coord_sub = pos['CCONJ'] + pos['SCONJ']\n",
    "    app = pos['ADP']\n",
    "    avv = pos['ADV']\n",
    "    determ = pos['DET']\n",
    "    st1 = nomi / verbi\n",
    "    st2 = coord_sub / verbi\n",
    "    st3 = app / verbi\n",
    "    st4 = avv / verbi\n",
    "    st5 = pronomi / nomi\n",
    "    st6 = determ / nomi\n",
    "    st7 = aggettivi / nomi\n",
    "    st8 = spazi / verbi\n",
    "    met = st1 + 2 * st2 + st3 + st4 + st5 * 0.8 + st6 * 0.8 + st7 * 0.5 - 0.7 * st8\n",
    "    return met\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_boardgame(rulebook): #use bgg api's to download data of each boardgame from the rulebook\n",
    "    req_for_id = bggxmlapi2 + \"search?query=\" + rulebook.filename.split(\".\")[0] + \"&type=boardgame&exact=1\"\n",
    "    res_for_id = requests.get(req_for_id)\n",
    "    root_for_id = et.fromstring(res_for_id.text)\n",
    "    game_id = root_for_id[0].get('id')\n",
    "    req_for_game = bggxmlapi + \"boardgame/\" + game_id\n",
    "    res_for_game = requests.get(req_for_game + \"?comments=1&stats=1\")\n",
    "    game = et.fromstring(res_for_game.text)[0]\n",
    "    name = game.find(\"name[@primary='true']\")\n",
    "    weight = game.find(\"./statistics/ratings/averageweight\")\n",
    "    expansions = game.findall(\"boardgameexpansion\")\n",
    "    comments = []\n",
    "\n",
    "    for comment in game.findall(\"./comment\"):\n",
    "        comments.append(comment.text)\n",
    "    eng_comments=select_english_comments(comments)\n",
    "    score=analyze_comments(eng_comments)\n",
    "\n",
    "    return BoardGame(name.text, eng_comments, weight.text, len(expansions), rulebook,score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_english_comments(comments): #to select only comments in english\n",
    "    eng_comm=[]\n",
    "    for c in comments:\n",
    "        doc = nlp(c)\n",
    "        eng_comm.append({\"comm\": c, \"lang\": doc._.language})\n",
    "    return [x[\"comm\"] for x in eng_comm if x[\"lang\"]==\"en\" ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_comments(comments): #creation of the metric from users' comments\n",
    "    I = defaultdict(set)\n",
    "    nltk_tokenize = lambda text: [x.lower() for x in nltk.word_tokenize(text) if x not in punctuation]\n",
    "    corpus = list(enumerate(comments))\n",
    "    for i, document in corpus:\n",
    "        tokens = nltk_tokenize(document.lower())\n",
    "        for token in tokens:\n",
    "            I[token].add(i)\n",
    "\n",
    "    easy_word_occurrence=0\n",
    "    for word in easy_word:\n",
    "        easy_word_occurrence += len(I[word])\n",
    "\n",
    "    difficult_word_occurrence = 0\n",
    "    for word in difficult_word:\n",
    "        difficult_word_occurrence += len(I[word])\n",
    "\n",
    "    B = defaultdict(lambda: 0)\n",
    "\n",
    "    for doc in comments:\n",
    "        for a, b in nltk.ngrams(nltk_tokenize(doc), 2):\n",
    "            B[(a, b)] += 1\n",
    "\n",
    "    difficult_word_occurrence_neg = 0\n",
    "    for word in difficult_word:\n",
    "        difficult_word_occurrence_neg += B[(\"not\", word)]\n",
    "\n",
    "    easy_word_occurrence_neg = 0\n",
    "    for word in easy_word:\n",
    "        easy_word_occurrence_neg += B[(\"not\", word)]\n",
    "\n",
    "    easy_word_occurrence_tot=easy_word_occurrence-easy_word_occurrence_neg\n",
    "    difficult_word_occurrence_tot=difficult_word_occurrence-difficult_word_occurrence_neg\n",
    "\n",
    "    if easy_word_occurrence_tot/difficult_word_occurrence_tot > 1.5:\n",
    "        score=-2\n",
    "    elif difficult_word_occurrence_tot/easy_word_occurrence_tot > 1.5:\n",
    "        score=1\n",
    "    else:\n",
    "        score=0\n",
    "\n",
    "    return score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATAN 2.3032 6.415468242213069\n",
      "Die Macher 4.3144 9.141517262128437\n",
      "Samurai 2.4794 6.562581847635009\n"
     ]
    }
   ],
   "source": [
    "files = read_files()\n",
    "boardgames=[]\n",
    "\n",
    "for file in files:\n",
    "    rulebook=read_rolebook_pdf(file)\n",
    "    boardgame=create_boardgame(rulebook)\n",
    "    boardgames.append(boardgame)\n",
    "\n",
    "\n",
    "for b in boardgames:\n",
    "    b.total_weight=b.rulebook.readability+ b.rulebook.syn_analysis+b.comment_score\n",
    "    if b.expansions_num>6:\n",
    "        b.total_weight=b.total_weight+1\n",
    "    print(b.name, b.BGGweight, b.total_weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
